{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "35e77e94-a5ee-4e86-bc1d-d4f652d04953",
      "metadata": {
        "id": "35e77e94-a5ee-4e86-bc1d-d4f652d04953"
      },
      "source": [
        "# OpenAI Assistants API (Beta) - File Search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "770a1d4f-8d1f-4b95-a5e6-4ab8d4ae5e17",
      "metadata": {
        "id": "770a1d4f-8d1f-4b95-a5e6-4ab8d4ae5e17"
      },
      "source": [
        "### import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0441db10-1aa1-4877-949e-166636bf145a",
      "metadata": {
        "id": "0441db10-1aa1-4877-949e-166636bf145a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# prompt: import secret key from colab named \"YOUR_OPENAI_API_KEY \"\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the API key from Colab secrets\n",
        "api_key = userdata.get('YOUR_OPENAI_API_KEY')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time"
      ],
      "metadata": {
        "id": "NVDwGxaV1I1y"
      },
      "id": "NVDwGxaV1I1y",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2050a44a-ebca-4dd7-8214-2377b3b5db85",
      "metadata": {
        "id": "2050a44a-ebca-4dd7-8214-2377b3b5db85"
      },
      "outputs": [],
      "source": [
        "openai.api_key = api_key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ae0f70-7224-44f7-b0ff-39ea3e6498c0",
      "metadata": {
        "id": "68ae0f70-7224-44f7-b0ff-39ea3e6498c0"
      },
      "source": [
        "### create client"
      ]
    },
    {
      "source": [
        "import openai\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Get the API key from Colab secrets (Assuming you've stored your key correctly)\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('YOUR_OPENAI_API_KEY')\n",
        "\n",
        "# Set the OPENAI_API_KEY environment variable\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "# **Instantiate the client object:**\n",
        "client = openai.Client()\n",
        "\n",
        "intstructions_string = \"HNMGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and concludes with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\"\n",
        "\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"HNM-GPT\",\n",
        "    description=\"Data scientist GPT for YouTube comments\",\n",
        "    instructions=intstructions_string,\n",
        "    model=\"gpt-4-0125-preview\"\n",
        ")\n",
        "print(assistant)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MPqYIgQK6v0S",
        "outputId": "68b33186-3735-42c7-8317-3d0e63f18e6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MPqYIgQK6v0S",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant(id='asst_CAugAUXJ3ksZH1YDqqekPS6d', created_at=1741691880, description='Data scientist GPT for YouTube comments', instructions=\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and concludes with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\", metadata={}, model='gpt-4-0125-preview', name='HNM-GPT', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0, reasoning_effort=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e33ed25-abfa-41c6-9c50-31b9bc03c3b5",
      "metadata": {
        "id": "3e33ed25-abfa-41c6-9c50-31b9bc03c3b5"
      },
      "source": [
        "### helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b28fb6f0-08e8-4f10-b6fc-1650186ea7eb",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "b28fb6f0-08e8-4f10-b6fc-1650186ea7eb"
      },
      "outputs": [],
      "source": [
        "def wait_for_assistant(thread, run):\n",
        "    \"\"\"\n",
        "        Function to periodically check run status of AI assistant and print run time\n",
        "    \"\"\"\n",
        "\n",
        "    # wait for assistant process prompt\n",
        "    t0 = time.time()\n",
        "    while run.status != 'completed':\n",
        "\n",
        "        # retreive status of run (this might take a few seconds or more)\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "          thread_id=thread.id,\n",
        "          run_id=run.id\n",
        "        )\n",
        "\n",
        "        # wait 0.5 seconds\n",
        "        time.sleep(0.25)\n",
        "    dt = time.time() - t0\n",
        "    print(\"Elapsed time: \" + str(dt) + \" seconds\")\n",
        "\n",
        "    return run"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "285aadc1-fcd6-4961-8d19-cbd88bc87115",
      "metadata": {
        "id": "285aadc1-fcd6-4961-8d19-cbd88bc87115"
      },
      "source": [
        "## Vanilla Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e094a2-7f03-4372-b921-9258a6bfe909",
      "metadata": {
        "id": "b0e094a2-7f03-4372-b921-9258a6bfe909"
      },
      "source": [
        "### create assistant\n",
        "List of available models: https://platform.openai.com/docs/models/continuous-model-upgrades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "54006d29-301a-4f99-ab2a-598c614e8c2f",
      "metadata": {
        "tags": [],
        "id": "54006d29-301a-4f99-ab2a-598c614e8c2f",
        "outputId": "741b66dd-5109-4e01-ca58-41c7a4530314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant(id='asst_N38d4ZRP28FJt1V9PzMzQqVq', created_at=1741691929, description='Data scientist GPT for YouTube comments', instructions=\"HNMGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and concludes with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\", metadata={}, model='gpt-4-0125-preview', name='HNM-GPT', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0, reasoning_effort=None)\n"
          ]
        }
      ],
      "source": [
        "intstructions_string = \"HNMGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and concludes with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\"\n",
        "\n",
        "assistant = client.beta.assistants.create(\n",
        "    name=\"HNM-GPT\",\n",
        "    description=\"Data scientist GPT for YouTube comments\",\n",
        "    instructions=intstructions_string,\n",
        "    model=\"gpt-4-0125-preview\"\n",
        ")\n",
        "print(assistant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "47a56d80-28be-41b5-bf53-5374503fb8c0",
      "metadata": {
        "tags": [],
        "id": "47a56d80-28be-41b5-bf53-5374503fb8c0"
      },
      "outputs": [],
      "source": [
        "# create thread (i.e. object that handles conversations between user and assistant)\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# generate user message\n",
        "user_message = \"Great content, thank you!\"\n",
        "\n",
        "# add a user message to the thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=user_message\n",
        ")\n",
        "\n",
        "# send message to assistant to generate a response\n",
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "426f9e3e-03c1-4997-9620-b64a6ea31df3",
      "metadata": {
        "id": "426f9e3e-03c1-4997-9620-b64a6ea31df3",
        "outputId": "26983089-9d26-4a62-8abd-ecf5f8bdd71b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 0.5667567253112793 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'run_LZRq9VAn1MTgEsCiJlKEwLQJ',\n",
              " 'assistant_id': 'asst_N38d4ZRP28FJt1V9PzMzQqVq',\n",
              " 'cancelled_at': None,\n",
              " 'completed_at': 1741691946,\n",
              " 'created_at': 1741691942,\n",
              " 'expires_at': None,\n",
              " 'failed_at': None,\n",
              " 'incomplete_details': None,\n",
              " 'instructions': \"HNMGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and concludes with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\",\n",
              " 'last_error': None,\n",
              " 'max_completion_tokens': None,\n",
              " 'max_prompt_tokens': None,\n",
              " 'metadata': {},\n",
              " 'model': 'gpt-4-0125-preview',\n",
              " 'object': 'thread.run',\n",
              " 'parallel_tool_calls': True,\n",
              " 'required_action': None,\n",
              " 'response_format': 'auto',\n",
              " 'started_at': 1741691943,\n",
              " 'status': 'completed',\n",
              " 'thread_id': 'thread_N2DED5CloXxaoA4LPimS9EgS',\n",
              " 'tool_choice': 'auto',\n",
              " 'tools': [],\n",
              " 'truncation_strategy': TruncationStrategy(type='auto', last_messages=None),\n",
              " 'usage': Usage(completion_tokens=32, prompt_tokens=108, total_tokens=140, prompt_token_details={'cached_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0}),\n",
              " 'temperature': 1.0,\n",
              " 'top_p': 1.0,\n",
              " 'tool_resources': {},\n",
              " 'reasoning_effort': None}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# wait for assistant process prompt\n",
        "run = wait_for_assistant(thread, run)\n",
        "\n",
        "# view run object (in Jupyter Lab)\n",
        "dict(run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b330bc4c-979e-462e-b6db-974417505980",
      "metadata": {
        "id": "b330bc4c-979e-462e-b6db-974417505980",
        "outputId": "e4993a6a-5998-46cc-a663-49b772893824",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're welcome! If you have any questions or need further information, feel free to ask. I'm here to help. –ShawGPT\n"
          ]
        }
      ],
      "source": [
        "# view messages added to thread\n",
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")\n",
        "\n",
        "print(messages.data[0].content[0].text.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "11692aa5-ae26-41af-9d07-40869f92d454",
      "metadata": {
        "id": "11692aa5-ae26-41af-9d07-40869f92d454",
        "outputId": "96824813-d256-4856-d609-7cb0df1a6887",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AssistantDeleted(id='asst_N38d4ZRP28FJt1V9PzMzQqVq', deleted=True, object='assistant.deleted')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# delete assistant\n",
        "client.beta.assistants.delete(assistant.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99c7c346-ecfb-4ee3-bbcd-b55f30549610",
      "metadata": {
        "id": "99c7c346-ecfb-4ee3-bbcd-b55f30549610"
      },
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4ea5bced-51fc-4f1c-bf15-c2016951e835",
      "metadata": {
        "tags": [],
        "id": "4ea5bced-51fc-4f1c-bf15-c2016951e835"
      },
      "outputs": [],
      "source": [
        "intstructions_string_few_shot = \"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and concludes with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "Here are examples of ShawGPT responding to viewer comments.\n",
        "\n",
        "Viewer comment: This was a very thorough introduction to LLMs and answered many questions I had. Thank you.\n",
        "ShawGPT: Great to hear, glad it was helpful :) -ShawGPT\n",
        "\n",
        "Viewer comment: Epic, very useful for my BCI class\n",
        "ShawGPT: Thanks, glad to hear! -ShawGPT\n",
        "\n",
        "Viewer comment: Honestly the most straightforward explanation I've ever watched. Super excellent work Shaw. Thank you. It's so rare to find good communicators like you!\n",
        "ShawGPT: Thanks, glad it was clear -ShawGPT\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "000a7f65-c724-4a79-a9c1-383acc5d88f3",
      "metadata": {
        "tags": [],
        "id": "000a7f65-c724-4a79-a9c1-383acc5d88f3"
      },
      "outputs": [],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=\"HNMGPT\",\n",
        "    description=\"Data scientist GPT for YouTube comments\",\n",
        "    instructions=intstructions_string_few_shot,\n",
        "    model=\"gpt-4-0125-preview\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e951f4b2-f616-45d0-98f8-76a4d55f8cd2",
      "metadata": {
        "tags": [],
        "id": "e951f4b2-f616-45d0-98f8-76a4d55f8cd2",
        "outputId": "3fc7307f-0937-4dc5-9fcd-4ba4fc361645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 3.6256461143493652 seconds\n"
          ]
        }
      ],
      "source": [
        "# create new thread\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# generate technical question\n",
        "user_message = \"Great content, thank you!\"\n",
        "\n",
        "# add a user message to the thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=user_message\n",
        ")\n",
        "\n",
        "# send message to assistant to generate a response (this might take a few seconds or more)\n",
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        ")\n",
        "\n",
        "# wait for assistant process prompt\n",
        "run = wait_for_assistant(thread, run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "127d5e26-8fdc-497a-8b8d-846fce0feaee",
      "metadata": {
        "tags": [],
        "id": "127d5e26-8fdc-497a-8b8d-846fce0feaee",
        "outputId": "830af58c-f832-4d68-db91-0c64c21d4b40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You're welcome, happy to help! -ShawGPT\n"
          ]
        }
      ],
      "source": [
        "# print assistant response\n",
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")\n",
        "\n",
        "print(messages.data[0].content[0].text.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "370f1826-fee4-4fc6-9e30-257b81984726",
      "metadata": {
        "id": "370f1826-fee4-4fc6-9e30-257b81984726"
      },
      "source": [
        "#### technical question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8579192c-977f-4cc5-8ece-2d639dd4f3eb",
      "metadata": {
        "id": "8579192c-977f-4cc5-8ece-2d639dd4f3eb"
      },
      "outputs": [],
      "source": [
        "# create new thread\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# generate technical question\n",
        "user_message = \"What is fat-tailedness?\"\n",
        "\n",
        "# add a user message to the thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=user_message\n",
        ")\n",
        "\n",
        "# send message to assistant to generate a response\n",
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "bb1b3864-44ac-411c-8090-3d84ddb46461",
      "metadata": {
        "id": "bb1b3864-44ac-411c-8090-3d84ddb46461",
        "outputId": "36880d5c-bbd1-4c5b-b160-cec9ae23ae95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 4.3396666049957275 seconds\n"
          ]
        }
      ],
      "source": [
        "# wait for assistant process prompt\n",
        "run = wait_for_assistant(thread, run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "93cdbdad-89d1-4648-b4ac-790a84b320fc",
      "metadata": {
        "id": "93cdbdad-89d1-4648-b4ac-790a84b320fc",
        "outputId": "51b6a354-1eb5-41f8-bfb1-537a345c5107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fat-tailedness refers to a statistical property of a distribution that exhibits extreme skewness or kurtosis, indicating an unusually high probability of obtaining values far away from the mean compared to a normal (or Gaussian) distribution. In simpler terms, a fat-tailed distribution has \"heavier\" tails, meaning it is more prone to producing outliers or extreme values. This characteristic is significant in various fields, such as finance, risk management, and insurance, because it suggests that extreme events (like financial crashes or natural disasters) are more likely to occur than would be predicted by models assuming a normal distribution. Understanding fat-tailedness helps in better assessing risks and preparing for potentially impactful rare events. -ShawGPT\n"
          ]
        }
      ],
      "source": [
        "# print assistant response\n",
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")\n",
        "\n",
        "print(messages.data[0].content[0].text.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "89b85d3c-a253-4224-a37f-758ddbf6da4b",
      "metadata": {
        "id": "89b85d3c-a253-4224-a37f-758ddbf6da4b",
        "outputId": "67f9e893-a157-463d-9cac-2199ee76e381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AssistantDeleted(id='asst_1TfvvKaaEcqtkF9zG8nrMSBo', deleted=True, object='assistant.deleted')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# delete assistant\n",
        "client.beta.assistants.delete(assistant.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927f7856-81fb-4f26-ab01-94010d9194cc",
      "metadata": {
        "tags": [],
        "id": "927f7856-81fb-4f26-ab01-94010d9194cc"
      },
      "source": [
        "## RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9bad5ed-9217-4181-a56b-b0f5861c9d57",
      "metadata": {
        "id": "c9bad5ed-9217-4181-a56b-b0f5861c9d57"
      },
      "source": [
        "#### add docs for retreival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4e4892d7-4f5d-4d96-939f-5508a67a12cb",
      "metadata": {
        "tags": [],
        "id": "4e4892d7-4f5d-4d96-939f-5508a67a12cb"
      },
      "outputs": [],
      "source": [
        "# create file (note: this will create a presisting file for your openai account, so be mindful about how many times you run this.\n",
        "# You can delete unnecessary files in the \"Files\" tab of your openai account.\n",
        "\n",
        "file = client.files.create(\n",
        "  file=open(\"/content/A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT.pdf\", \"rb\"),\n",
        "  purpose=\"assistants\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23c84ca3-af9d-48bb-8792-b3708ddd7648",
      "metadata": {
        "id": "23c84ca3-af9d-48bb-8792-b3708ddd7648"
      },
      "source": [
        "#### create new assistant with access to docs"
      ]
    },
    {
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=\"HNMGPT\",\n",
        "    description=\"Data scientist GPT for YouTube comments\",\n",
        "    instructions=intstructions_string_few_shot,\n",
        "    tools=[{\"type\": \"file_search\"}],  # Changed 'retrieval' to 'file_search'\n",
        "    model=\"gpt-4-0125-preview\"\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2gtRL4VL_zYr"
      },
      "id": "2gtRL4VL_zYr",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4067eac9-f940-46d2-895b-6da2a9b2e045",
      "metadata": {
        "id": "4067eac9-f940-46d2-895b-6da2a9b2e045"
      },
      "source": [
        "#### technical question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "7d274d21-33bc-45af-89ea-bef11af48c39",
      "metadata": {
        "tags": [],
        "id": "7d274d21-33bc-45af-89ea-bef11af48c39",
        "outputId": "0d431e39-1e84-4bc7-9f1d-db83a76711e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 16.196894884109497 seconds\n"
          ]
        }
      ],
      "source": [
        "# create new thread\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# generate technical question\n",
        "user_message = \"What is Prompt Engineering\"\n",
        "\n",
        "# add a user message to the thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=user_message\n",
        ")\n",
        "\n",
        "# send message to assistant to generate a response (this might take a several seconds or more)\n",
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        ")\n",
        "\n",
        "# wait for assistant process prompt\n",
        "run = wait_for_assistant(thread, run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "622b6f7a-1007-496b-8cf0-6bf966a99a83",
      "metadata": {
        "tags": [],
        "id": "622b6f7a-1007-496b-8cf0-6bf966a99a83",
        "outputId": "ed42058c-265c-4e97-f2c5-4d9a10ba6181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt engineering is the process of designing and refining prompts (i.e., the inputs or questions) given to artificial intelligence (AI) models, especially large language models (LLMs) like GPT-3 or GPT-4, to elicit the most accurate, relevant, or creative responses possible. It's a critical skill in the utilization of AI for various applications, such as content creation, programming, data analysis, and more. The goal of prompt engineering is to understand how to communicate effectively with these AI systems to leverage their capabilities fully.\n",
            "\n",
            "Here’s a breakdown of what it entails and why it's important:\n",
            "\n",
            "### Components of Prompt Engineering\n",
            "1. **Prompt Design:** The initial creation of a prompt that clearly and effectively communicates the task or information request to the AI.\n",
            "2. **Iterative Refinement:** Adjusting the prompt based on the AI’s responses to achieve better outcomes. This could involve specifying the format of the desired response, adding context or constraints, or asking follow-up questions.\n",
            "3. **Understanding AI Capabilities:** Knowing what the AI model can do and its limitations helps in crafting prompts that are more likely to result in useful responses.\n",
            "4. **Specificity and Clarity:** Being specific and clear in what is asked of the AI to avoid ambiguous or incorrect answers.\n",
            "\n",
            "### Importance of Prompt Engineering\n",
            "- **Maximizes Efficiency:** Properly engineered prompts can significantly decrease the time and resources needed to obtain useful outputs from AI models.\n",
            "- **Enhances Accuracy:** Tailored prompts lead to more accurate and relevant responses, reducing the need for human correction.\n",
            "- **Expands Use Cases:** Skillful prompt engineering can unlock new applications for AI technology by effectively utilizing the model’s capabilities.\n",
            "\n",
            "### Strategies for Effective Prompt Engineering\n",
            "- **Chain of Thought Prompting:** Encouraging the AI to \"show its work\" or explain the steps it took to arrive at an answer, enhancing transparency and reliability of the response.\n",
            "- **Zero-Shot and Few-Shot Learning:** Crafting prompts that enable the AI to perform tasks or answer questions it has not been explicitly trained on, leveraging its learned patterns and knowledge.\n",
            "- **Prompt Templates:** Developing templates for similar types of tasks or questions can streamline the process of using AI for various activities.\n",
            "\n",
            "As AI technology evolves, the role of prompt engineering grows in importance, serving as a bridge between human objectives and AI capabilities. Effective prompt engineering can significantly enhance the utility and efficiency of AI tools in solving complex problems and generating innovative solutions.\n",
            "\n",
            "–ShawGPT\n"
          ]
        }
      ],
      "source": [
        "# print assistant response\n",
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")\n",
        "\n",
        "print(messages.data[0].content[0].text.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "c473baee-729d-4cb9-9e0c-3bce0e64d9ff",
      "metadata": {
        "id": "c473baee-729d-4cb9-9e0c-3bce0e64d9ff",
        "outputId": "e01aebb7-c0df-4796-940d-250277d468e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AssistantDeleted(id='asst_o6kinhgKxayOq780QKtwsSyq', deleted=True, object='assistant.deleted')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# delete assistant\n",
        "client.beta.assistants.delete(assistant.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "83fcf324-f529-48c3-b5c4-f044ee12805d",
      "metadata": {
        "id": "83fcf324-f529-48c3-b5c4-f044ee12805d",
        "outputId": "8cefabed-3b99-4599-d37c-6a041afa0290",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileDeleted(id='file-9QSkLgmXRLSgNE3EgQrVAH', deleted=True, object='file')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# delete file\n",
        "client.files.delete(file.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fe2c71f-91e5-4919-9c06-dc303ad5f544",
      "metadata": {
        "id": "8fe2c71f-91e5-4919-9c06-dc303ad5f544"
      },
      "source": [
        "### More Resources\n",
        "\n",
        "Assistants API: https://platform.openai.com/docs/assistants/overview <br>\n",
        "Assistants Doc: https://platform.openai.com/docs/api-reference/assistants <br>\n",
        "More on tools: https://platform.openai.com/docs/assistants/tools/code-interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f776bf-2361-4435-b7df-107ea0bea148",
      "metadata": {
        "id": "e1f776bf-2361-4435-b7df-107ea0bea148"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}